{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de0a7ff-b279-4bad-9c87-60154dab913d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51d05b07-9dbd-4fb4-bf8f-620070f1fab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Unjudged@10</th>\n",
       "      <th>nDCG@10</th>\n",
       "      <th>condensed-nDCG@10</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bm25</td>\n",
       "      <td>0.760130</td>\n",
       "      <td>0.183680</td>\n",
       "      <td>0.477587</td>\n",
       "      <td>0.265876</td>\n",
       "      <td>Sept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BM25-5-variants-prompt-2</td>\n",
       "      <td>0.791549</td>\n",
       "      <td>0.164992</td>\n",
       "      <td>0.472690</td>\n",
       "      <td>0.244134</td>\n",
       "      <td>Sept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BM25-10-variants-prompt-2</td>\n",
       "      <td>0.786349</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.478501</td>\n",
       "      <td>0.248608</td>\n",
       "      <td>Sept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>LGD-10-variants-prompt-2</td>\n",
       "      <td>0.790791</td>\n",
       "      <td>0.166178</td>\n",
       "      <td>0.482670</td>\n",
       "      <td>0.254577</td>\n",
       "      <td>Sept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>PL2-5-variants-prompt-2</td>\n",
       "      <td>0.789707</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.485604</td>\n",
       "      <td>0.250316</td>\n",
       "      <td>Sept.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>PL2-10-variants-prompt-2</td>\n",
       "      <td>0.788082</td>\n",
       "      <td>0.166765</td>\n",
       "      <td>0.487811</td>\n",
       "      <td>0.252517</td>\n",
       "      <td>Sept.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  Unjudged@10   nDCG@10  condensed-nDCG@10  \\\n",
       "31                       bm25     0.760130  0.183680           0.477587   \n",
       "36   BM25-5-variants-prompt-2     0.791549  0.164992           0.472690   \n",
       "37  BM25-10-variants-prompt-2     0.786349  0.169106           0.478501   \n",
       "55   LGD-10-variants-prompt-2     0.790791  0.166178           0.482670   \n",
       "60    PL2-5-variants-prompt-2     0.789707  0.164181           0.485604   \n",
       "61   PL2-10-variants-prompt-2     0.788082  0.166765           0.487811   \n",
       "\n",
       "         MRR Dataset  \n",
       "31  0.265876   Sept.  \n",
       "36  0.244134   Sept.  \n",
       "37  0.248608   Sept.  \n",
       "55  0.254577   Sept.  \n",
       "60  0.250316   Sept.  \n",
       "61  0.252517   Sept.  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_to_pretty_name = {\n",
    "    'longeval-heldout-20230513-training': 'June',\n",
    "    'longeval-long-september-20230513-training': 'Sept.',\n",
    "    'longeval-short-july-20230513-training': 'July',\n",
    "}\n",
    "\n",
    "df = pd.read_json('complete-evaluation.jsonl', lines=True)\n",
    "df['Dataset'] = df['Dataset'].apply(lambda i: dataset_to_pretty_name.get(i))\n",
    "df = df[df['name'].str.lower().isin(['bm25', 'bm25-5-variants-prompt-2', 'bm25-10-variants-prompt-2', 'lgd-10-variants-prompt-2', 'pl2-5-variants-prompt-2', 'pl2-10-variants-prompt-2'])]\n",
    "df[df['Dataset'] == 'Sept.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0221a-ca8d-4083-ac36-ac4a9ba6d146",
   "metadata": {},
   "source": [
    "# Creation of the Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d908414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trectools\n",
      "  Downloading trectools-0.0.49.tar.gz (28 kB)\n",
      "Building wheels for collected packages: trectools\n",
      "  Building wheel for trectools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for trectools: filename=trectools-0.0.49-py3-none-any.whl size=27139 sha256=050c6b04c5007a86a1acf7961858a823f9220cda585ee02520d911c0bb021d21\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/cd/17/9a6b28af70445d948c97018b43b9181acd2fdd23e115ee2055\n",
      "Successfully built trectools\n",
      "Installing collected packages: trectools\n",
      "Successfully installed trectools-0.0.49\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sarge\n",
      "  Downloading sarge-0.1.7.post1-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sarge\n",
      "Successfully installed sarge-0.1.7.post1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install trectools --no-deps\n",
    "!pip3 install sarge --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f7513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.rest_api_client import Client\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ir_datasets\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from trectools import TrecRun, TrecEval, TrecQrel\n",
    "\n",
    "tira = Client()\n",
    "\n",
    "TASK = 'ir-benchmarks'\n",
    "TEAM = 'ows'\n",
    "    \n",
    "datasets = ['longeval-heldout-20230513-training', 'longeval-long-september-20230513-training',\n",
    "            'longeval-short-july-20230513-training', 'longeval-train-20230513-training']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6f8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrels(dataset):\n",
    "    ds = ir_datasets.load(dataset)\n",
    "    ds = pd.DataFrame([{\"query\": i.query_id, \"q0\": \"0\", \"docid\": i.doc_id, \"rel\": i.relevance} for i in ds.qrels_iter()])\n",
    "    ret = TrecQrel()\n",
    "    ret.qrels_data = ds\n",
    "    \n",
    "    return ret\n",
    "\n",
    "QRELS = {\n",
    "    'longeval-heldout-20230513-training': load_qrels('longeval/heldout'),\n",
    "    'longeval-long-september-20230513-training': load_qrels('longeval/b-long-september'),\n",
    "    'longeval-short-july-20230513-training': load_qrels('longeval/a-short-july'),\n",
    "    'longeval-train-20230513-training': load_qrels('longeval/train'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0276cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_effectiveness(name, run, dataset):\n",
    "    te = TrecEval(run, QRELS[dataset])\n",
    "    \n",
    "    return {'name': name,\n",
    "            'Unjudged@10': te.get_unjudged(depth=10),\n",
    "            'nDCG@10': te.get_ndcg(depth=10, removeUnjudged=False),\n",
    "            'condensed-nDCG@10': te.get_ndcg(depth=10, removeUnjudged=True),\n",
    "            'MRR': te.get_reciprocal_rank(removeUnjudged=True),\n",
    "            'Dataset': dataset,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b50ca073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation on longeval-heldout-20230513-training: 100%|█████████████████████████████████████████████████████████████| 5/5 [00:46<00:00,  9.32s/it]\n",
      "Evaluation on longeval-long-september-20230513-training: 100%|██████████████████████████████████████████████████████| 5/5 [07:21<00:00, 88.24s/it]\n",
      "Evaluation on longeval-short-july-20230513-training: 100%|██████████████████████████████████████████████████████████| 5/5 [07:29<00:00, 89.94s/it]\n",
      "Evaluation on longeval-train-20230513-training: 100%|███████████████████████████████████████████████████████████████| 5/5 [05:24<00:00, 64.86s/it]\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    bm25 = TrecRun(tira.get_run_output(f'{TASK}/{TEAM}/PyTerrier-Index >> BM25', dataset) + '/run.txt')\n",
    "    df += [report_effectiveness('bm25', bm25, dataset)]\n",
    "    \n",
    "    for wmodel in tqdm(['BM25', 'DPH', 'DirichletLM', 'LGD', 'PL2'], f'Evaluation on {dataset}'):\n",
    "        for prompt in ['1', '2']:\n",
    "            for variants in ['3', '5', '10']:\n",
    "                run = TrecRun(f'all-results/{dataset}/{wmodel}-{variants}-variants-prompt-{prompt}-fused-run.txt.gz')\n",
    "            \n",
    "                df += [report_effectiveness(f'{wmodel}-{variants}-variants-prompt-{prompt}', run, dataset)]\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89fdc8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('complete-evaluation.jsonl', lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44e5ac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--    1 root     users      18.6K Jun  6 09:21 \u001b[0;0mcomplete-evaluation.jsonl\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -lh complete-evaluation.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
